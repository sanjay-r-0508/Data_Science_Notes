{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2110d6d7",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "- A cost function is an important parameter that determines how well the machine learning algorithms or models performs for a given dataset.\n",
    "\n",
    "- cost function is a measure of how wrong the model is estimating the relationship between X and Y paramters.\n",
    "\n",
    "##### Types of Cost Function:\n",
    "\n",
    "1. Regression Cost Function\n",
    "2. Classification Cost Function\n",
    "\n",
    "##### 1. Regression cost function\n",
    "\n",
    "Regression models are used to make a prediction for the continuous variables. \n",
    "\n",
    "- MSE (Mean Squared Error)\n",
    "- RMSE (Root mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- R^2 Accuracy\n",
    "\n",
    "##### 2. Binary classification Cost Functions:\n",
    "\n",
    "classification models are used to make predictions of categorical variables, such as predictions for 0 or 1, Cat or Dog, etc.\n",
    "\n",
    "###### 3. Multi-class classification Cost Functions:\n",
    "\n",
    "A multi-class classification cost function is used in the classification problems for which instances are allocated to one or more than two classes.\n",
    "\n",
    "- Binary Cross Entropy Cost Function or Log Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3869e9a",
   "metadata": {},
   "source": [
    "# Regression Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e421d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a030859f",
   "metadata": {},
   "source": [
    "# Regularization Technique\n",
    "\n",
    "- This is a forrm of regularization, that contrains / regularizes or shrinks the coefficient estimates towards zero.\n",
    "\n",
    "- This technique discourages learning a more complex or flexible model, so as to aviod the risk of overfitting.\n",
    "\n",
    "##### Regularization can achieve this motive with 2 techniques:\n",
    "\n",
    "- Lasso Regularization / L1\n",
    "- Ridge Regularization / L2\n",
    "\n",
    "# 1. Lasso Regularization (L1):\n",
    "\n",
    "- This is a regularization technique used in feature selection using a shrinkage method also referred to as the penalized regression method.\n",
    "\n",
    "- Lasso Regression magnitude of coefficients can be exactly zero.\n",
    "\n",
    "                                        cost fuction  = Loss + lambda * sum || w ||\n",
    "\n",
    "                                        - Loss = sum of squared residual\n",
    "                                        - lambda = penalty\n",
    "                                        - w = slope of the curve\n",
    "# 2. Ridge Regularization (L2):\n",
    "\n",
    "- Ridge Regression, also known as L2 regularization, is an extension to linear Regression that introduces a regularization term to reduce model complexity and help prevent overfitting.\n",
    "\n",
    "- Ridge Regression is working value / magni8tude of coefficients is almost eqaul to zero.\n",
    "\n",
    "                                        cost fuction  = Loss + lambda * sum || w ||^2\n",
    "\n",
    "                                        - Loss = sum of squared residual\n",
    "                                        - lambda = penalty\n",
    "                                        - w = slope of the curve\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e4e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
